{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df9cfce-606a-4912-b667-020fa98f33fd",
   "metadata": {},
   "source": [
    "# Data Ingestion Process\n",
    "\n",
    "## Overview\n",
    "The data ingestion process is the first step in our pipeline, where raw data is loaded, validated, and prepared for analysis. This ensures that the dataset is reliable and ready for exploratory data analysis and modeling.\n",
    "\n",
    "## Key Steps\n",
    "1. **Reading Data**:\n",
    "   - The dataset was read from a CSV file located at `/data/Telco_customer_churn.csv`.\n",
    "   - Code snippet:\n",
    "     ```python\n",
    "     data = pd.read_csv(input_file_path)\n",
    "     print(f\"Data loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
    "     ```\n",
    "\n",
    "2. **Validation**:\n",
    "   - Performed basic validation checks:\n",
    "     - Checked for empty datasets: `if data.empty`.\n",
    "     - Checked for duplicate rows: `data.duplicated().sum()`.\n",
    "   - Summary:\n",
    "     - Rows: 7043, Columns: 33.\n",
    "     - Duplicate rows: 0.\n",
    "\n",
    "3. **Saving Processed Data**:\n",
    "   - The validated dataset was saved in the `/data/processed/` directory with a timestamp.\n",
    "   - Code snippet:\n",
    "     ```python\n",
    "     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "     output_file = os.path.join(output_dir, f\"ingested_data_{timestamp}.csv\")\n",
    "     data.to_csv(output_file, index=False)\n",
    "     ```\n",
    "\n",
    "## Results\n",
    "- Processed data saved successfully at `/data/processed/ingested_data_YYYYMMDD_HHMMSS.csv`.\n",
    "- Preview of the data:\n",
    "  ```plaintext\n",
    "     CustomerID  Count    Country     State         City  Zip Code ...\n",
    "  0  3668-QPYBK      1  United States  California  Los Angeles  90003 ...\n",
    "  1  9237-HQITU      1  United States  California  Los Angeles  90005 ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c29de4-75d4-45db-a09a-e9322f533a89",
   "metadata": {},
   "source": [
    "## Project Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ae1862-fe3c-4067-b321-5153b984dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb0062d-52d6-4d51-9e76-80b0513cf69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory set to: /data/processed/\n"
     ]
    }
   ],
   "source": [
    "# Defining input and output paths\n",
    "input_file_path = \"/data/Telco_customer_churn.csv\"  # Adjust the path as needed\n",
    "output_dir = \"/data/processed/\"\n",
    "\n",
    "# Creating the output directory (if it doesn't exist already)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory set to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf519c8-b947-41e8-b386-979f02c58e3d",
   "metadata": {},
   "source": [
    "##  Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d154e8d-77dd-4e6c-85e5-f55ae34db9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self, input_file_path, output_dir):\n",
    "        \"\"\"\n",
    "        Initializes the DataIngestion class with file path and output directory.\n",
    "\n",
    "        :param input_file_path: Path to the input CSV file.\n",
    "        :param output_dir: Directory to save the processed data.\n",
    "        \"\"\"\n",
    "        self.input_file_path = input_file_path\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def read_data(self):\n",
    "        \"\"\"\n",
    "        Reads data from the input file.\n",
    "\n",
    "        :return: Pandas DataFrame containing the dataset.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Reading data from {self.input_file_path}...\")\n",
    "            data = pd.read_csv(self.input_file_path)\n",
    "            print(f\"Data read successfully with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def validate_data(self, data):\n",
    "        \"\"\"\n",
    "        Performs basic validation checks on the dataset.\n",
    "\n",
    "        :param data: Pandas DataFrame.\n",
    "        \"\"\"\n",
    "        if data.empty:\n",
    "            raise ValueError(\"The dataset is empty. Please provide a valid file.\")\n",
    "        if not all(data.columns):\n",
    "            raise ValueError(\"Some columns have no names. Please check the file.\")\n",
    "        print(\"Basic validation checks passed.\")\n",
    "\n",
    "        # Example: Check for duplicates\n",
    "        if data.duplicated().sum() > 0:\n",
    "            print(f\"Warning: Dataset contains {data.duplicated().sum()} duplicate rows.\")\n",
    "        else:\n",
    "            print(\"No duplicate rows found.\")\n",
    "\n",
    "    def save_data(self, data):\n",
    "        \"\"\"\n",
    "        Saves the ingested data to the output directory with a timestamp.\n",
    "\n",
    "        :param data: Pandas DataFrame.\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = os.path.join(self.output_dir, f\"ingested_data_{timestamp}.csv\")\n",
    "        try:\n",
    "            data.to_csv(output_file, index=False)\n",
    "            print(f\"Data saved to {output_file}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the data ingestion pipeline.\n",
    "        \"\"\"\n",
    "        data = self.read_data()\n",
    "        self.validate_data(data)\n",
    "        self.save_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae26599-dc44-4da3-acac-e11ead89b5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from ../data/telco_customer_churn.csv...\n",
      "Data read successfully with 7043 rows and 33 columns.\n",
      "Basic validation checks passed.\n",
      "No duplicate rows found.\n",
      "Data saved to ../data/processed/ingested_data_20250105_192040.csv.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and run the ingestion pipeline\n",
    "ingestion = DataIngestion(input_file_path=input_file_path, output_dir=output_dir)\n",
    "ingestion.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f526d4-cd53-492b-b9f5-8d4ec2d117c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed files: ['../data/processed\\\\ingested_data_20250105_185658.csv', '../data/processed\\\\ingested_data_20250105_191921.csv', '../data/processed\\\\ingested_data_20250105_192040.csv']\n",
      "Loading the latest processed file: ../data/processed\\ingested_data_20250105_192040.csv\n",
      "   CustomerID  Count        Country       State         City  Zip Code  \\\n",
      "0  3668-QPYBK      1  United States  California  Los Angeles     90003   \n",
      "1  9237-HQITU      1  United States  California  Los Angeles     90005   \n",
      "2  9305-CDSKC      1  United States  California  Los Angeles     90006   \n",
      "3  7892-POOKP      1  United States  California  Los Angeles     90010   \n",
      "4  0280-XJGEX      1  United States  California  Los Angeles     90015   \n",
      "\n",
      "                 Lat Long   Latitude   Longitude  Gender  ...        Contract  \\\n",
      "0  33.964131, -118.272783  33.964131 -118.272783    Male  ...  Month-to-month   \n",
      "1   34.059281, -118.30742  34.059281 -118.307420  Female  ...  Month-to-month   \n",
      "2  34.048013, -118.293953  34.048013 -118.293953  Female  ...  Month-to-month   \n",
      "3  34.062125, -118.315709  34.062125 -118.315709  Female  ...  Month-to-month   \n",
      "4  34.039224, -118.266293  34.039224 -118.266293    Male  ...  Month-to-month   \n",
      "\n",
      "  Paperless Billing             Payment Method  Monthly Charges Total Charges  \\\n",
      "0               Yes               Mailed check            53.85        108.15   \n",
      "1               Yes           Electronic check            70.70        151.65   \n",
      "2               Yes           Electronic check            99.65         820.5   \n",
      "3               Yes           Electronic check           104.80       3046.05   \n",
      "4               Yes  Bank transfer (automatic)           103.70        5036.3   \n",
      "\n",
      "  Churn Label Churn Value Churn Score  CLTV                   Churn Reason  \n",
      "0         Yes           1          86  3239   Competitor made better offer  \n",
      "1         Yes           1          67  2701                          Moved  \n",
      "2         Yes           1          86  5372                          Moved  \n",
      "3         Yes           1          84  5003                          Moved  \n",
      "4         Yes           1          89  5340  Competitor had better devices  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "# Listing all files in the processed data directory\n",
    "import glob\n",
    "\n",
    "processed_files = glob.glob(os.path.join(output_dir, \"*.csv\"))\n",
    "print(\"Processed files:\", processed_files)\n",
    "\n",
    "# Loading the latest ingested file\n",
    "latest_file = max(processed_files, key=os.path.getctime)\n",
    "print(f\"Loading the latest processed file: {latest_file}\")\n",
    "processed_data = pd.read_csv(latest_file)\n",
    "print(processed_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579370e1-4fc0-4cd3-95b5-1aad4974a26a",
   "metadata": {},
   "source": [
    "### Inspect Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a10347cd-c131-4b62-a0a3-e39616edc89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CustomerID  Count        Country       State         City  Zip Code  \\\n",
      "0  3668-QPYBK      1  United States  California  Los Angeles     90003   \n",
      "1  9237-HQITU      1  United States  California  Los Angeles     90005   \n",
      "2  9305-CDSKC      1  United States  California  Los Angeles     90006   \n",
      "3  7892-POOKP      1  United States  California  Los Angeles     90010   \n",
      "4  0280-XJGEX      1  United States  California  Los Angeles     90015   \n",
      "\n",
      "                 Lat Long   Latitude   Longitude  Gender  ...        Contract  \\\n",
      "0  33.964131, -118.272783  33.964131 -118.272783    Male  ...  Month-to-month   \n",
      "1   34.059281, -118.30742  34.059281 -118.307420  Female  ...  Month-to-month   \n",
      "2  34.048013, -118.293953  34.048013 -118.293953  Female  ...  Month-to-month   \n",
      "3  34.062125, -118.315709  34.062125 -118.315709  Female  ...  Month-to-month   \n",
      "4  34.039224, -118.266293  34.039224 -118.266293    Male  ...  Month-to-month   \n",
      "\n",
      "  Paperless Billing             Payment Method  Monthly Charges Total Charges  \\\n",
      "0               Yes               Mailed check            53.85        108.15   \n",
      "1               Yes           Electronic check            70.70        151.65   \n",
      "2               Yes           Electronic check            99.65         820.5   \n",
      "3               Yes           Electronic check           104.80       3046.05   \n",
      "4               Yes  Bank transfer (automatic)           103.70        5036.3   \n",
      "\n",
      "  Churn Label Churn Value Churn Score  CLTV                   Churn Reason  \n",
      "0         Yes           1          86  3239   Competitor made better offer  \n",
      "1         Yes           1          67  2701                          Moved  \n",
      "2         Yes           1          86  5372                          Moved  \n",
      "3         Yes           1          84  5003                          Moved  \n",
      "4         Yes           1          89  5340  Competitor had better devices  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/processed/ingested_data_20250105_192040.csv\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5964b5-0b79-40eb-9186-5b77a39c47b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
